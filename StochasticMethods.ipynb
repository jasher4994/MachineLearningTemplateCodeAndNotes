{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic methods - helpful code and notes for Linear, Ridge, Lasso, Elastic Net, Logistic regression and SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This brief notebook looks at stochastic methods for analysis in machine learning. It focuses primarily on methods that use stochastic gradient descent as an optimizing function. It also looks briefly as linear regression as a building block for the rest of the methods. It looks at regularized versions of regression(Ridge, Lasso, ElasticNet) before adapting these to account for polynomial relationships. From here it moved onto classifier models -  logistic regression and SVM which are the next logical jump from linear regression models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package and pre-requesite import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "X = 2 * np.random.rand(100,1) #create a 100x1 vector of instances\n",
    "y = 4 + 3 * X + np.random.randn(100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.82929424] [[3.15667259]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 3.82929424],\n",
       "       [10.14263941]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X,y)\n",
    "print(lin_reg.intercept_, lin_reg.coef_)\n",
    "X_new = np.array([[0], [2]]) #predict requires a 2D array, can reshape data if only one feature but easier to just predict 2\n",
    "lin_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As can be seen, if Xi = 0 then y^ is simply equal to the intercept.\n",
    "- The scikit learn module uses SVD(Singular value decimposition) as the matrix factroring technique. \n",
    "- This approach is less computationaly complex than using the normal equation (On^2 >> ~~ On^3)\n",
    "- This will still be very slow as n > infinity.\n",
    "- Simple Linear regression also has no regularization and is therefore prone to overfitting. \n",
    "- Both the problems of overfitting and computational complexity can be solved by not using the normal equation(or similar) and using a gradient descent algorithm. There are three main types of regularized regressions which can be seen below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, it must be noted that stochastic methods are not a family of machine learning models and are simply an optimization technique.You can compute a linear regression without stochastic gradient descent by use of the normal equation but it is not reccomended. It is very rare that we will need to use an unregularized linear regression.\n",
    "\n",
    "Gradient descent has three main variations. Batch Gradient desent (full GD), Stochastic, or mini batch. Batch gradient descent goes through each individual obervation individually and adjusts the relevant coefficients. Stochastic, picks coefficients randomly each epoch and does the same, mini batch is the a hybrid of the two an choses small batches. Bath gradient descent of course takes the longest time but will converge the closest to exactly. Stochastic and mini batch will converge to effectively the same value if enough time is allowed. The advantages of the latter two techniques are that they are considerably faster. \n",
    "\n",
    "Gradient descent is sensitive to feature scaling (in terms of computational time) therefore, features should always be normalised or standardised before fitting the model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularised linear regression (Ridge,   Lasso, Elastic Net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three main type of regularized linear regressions ar Ridge, Lasso, Elastic net. These can all be accessed through Scikit Learms Stochastic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "ridge = SGDRegressor(loss=\"squared_loss\", penalty=\"l2\", max_iter=500) #ridge uses l2\n",
    "lasso = SGDRegressor(loss=\"squared_loss\", penalty=\"l1\", max_iter=500) #lasso uses l1\n",
    "elastic_net =  SGDRegressor(loss=\"squared_loss\", penalty=\"l2\", max_iter=500) #hybrid of l1 and l2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.46845465, 10.37826369])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge.fit(X,y.ravel()) #Ravel just flattens arrays to 1D, doesnt matter in this isntance but useful in future. \n",
    "ridge.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.46090492, 10.39331964])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso.fit(X,y.ravel()) #And it stops the ugly wanring coming up.\n",
    "lasso.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.47242528, 10.36771542])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elastic_net.fit(X,y.ravel()) \n",
    "elastic_net.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the results from all three models are very similar with such simple data. This will not be the case in the future where the data is more complicated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important arguments:\n",
    "- loss: default = \"squared loss\", alternatively \"huber\" modifies OLS to focus less on extreme outliers.\n",
    "- penalty: This is the regularisation extension to the loss function. l1 is the sum of absolutes (MAE) and is equivalent to the manhattan distance. This is used in Lasso regression. L2 (default) is the RMSE and is used in Ride, it is equivalent to the Euclidiean distance. \"elasticnet\" is a hybrid of both l1 and l2. The ratio is controlled by \"l1_ratio\" and defaults at 0.15.\n",
    "- alpha: The alpha is the constant that multiples the regularisation term, as alpha increases so does the regularisation. Defaults at 0.001.\n",
    "- Learning rate defaults to optimal.Should rarely be adjusted.\n",
    "- early_stopping: can be used to stop training when vaidation score stops improving. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important things to remember\n",
    "- Lasso regression and elastic net (with high ratio of L1) effectively does feature selection. It tends the least important variable coefficients towards zero. This is useful when you think that only a few features are actually important. \n",
    "- Instances must be idnetically and independantly distributed  - therefore,  shuffling is suggested (Default = shuffle)\n",
    "- ElasticNet is almost always preferred to lasso (even though they can be identical) because sometimes lasso can be arratic at high numbers of features and instances. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial regression expands on linear regression by allowing features to mix with each other and themselves. If there were two features A and B, a polynomial regression (degrees = 3) would consist of features a + b + ab + a^2 + b^2. Therefore, this allows the relationship to take any shape, not just a straight line. \n",
    "\n",
    "Polynomial regressions thereforesignificantly increase the chance of overfitting, so regularisation is a must. \n",
    "\n",
    "Apart from the relationships between features, the process is the same as linear regression. The most apprpriate way to get polynomial features is to use sklearns preprocessing module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  1.,  0.],\n",
       "       [ 1.,  2.,  3.,  6.],\n",
       "       [ 1.,  4.,  5., 20.]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "X = np.arange(6).reshape(3, 2)\n",
    "poly = PolynomialFeatures(2)\n",
    "poly.fit_transform(X)\n",
    "poly = PolynomialFeatures(interaction_only=True) #can chose interaction only (not exponentials)\n",
    "poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following this step, the models used before can be done the same way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logisitic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logisitic regression is also most commonly fitted using gradient descent algorithms - here is no closed form solution.\n",
    "- Just like Linear Regression, logistic regression computesa weighted sum of the input features. Instead, however, it outputs the logistic of the results.\n",
    "- Log reg can be (and should be) regularized in the same fashion as linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('standardscaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('sgdclassifier',\n",
       "                 SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "                               early_stopping=False, epsilon=0.1, eta0=0.0,\n",
       "                               fit_intercept=True, l1_ratio=0.15,\n",
       "                               learning_rate='optimal', loss='log',\n",
       "                               max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                               penalty='l2', power_t=0.5, random_state=None,\n",
       "                               shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "                               verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "#could also introduce polynomial varibles into the pipeline here too, fittransform the X.\n",
    "X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
    "Y = np.array([1, 1, 2, 2])\n",
    "# Always scale the input. The most convenient way is to use a pipeline.\n",
    "log_clf = make_pipeline(StandardScaler(),\n",
    "                        SGDClassifier(max_iter=1000, tol=1e-3, loss = \"log\")) #loss = log for logistic regression\n",
    "log_clf.fit(X, Y)\n",
    "\n",
    "#print(log_clf.predict([[-0.8, -1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, a lot of the argments are the same as the regularized linear regressions. Penalty is automatically set to l2, bu can be adjusted to \"elasticnet\" and the proportion can be set with \"l1_ratio\". Shuffle is also automatically assumed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Support vector machines can be employed in the same way as logistic regression by changing the loss to \"hinge\". \n",
    "- SVM are particularly useful in small to medium sized complex datasets. \n",
    "- SVM are storngly sensitive to feature scales to features must be standardised before. \n",
    "- Model can be regularized by controlling \"C\" - the \"width of the road\".\n",
    "- more flexibility can be achived with SVC model \"from sklearn imort SVC\".\n",
    "- SVM work on linear seperability, so therefore, introducint polynomial features works well here.\n",
    "- The \"kernel trick\" allows us to introdudce these polynomial features without actually doing so and therefore not requiring the computing power. \n",
    "- Similarilty functions are also a really useful way to add features - Guassian Radical bias function is the most commonly used.\n",
    "- The most common and simplest approach is to create a landmark for each instance. \n",
    "- Really good documentation here. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
